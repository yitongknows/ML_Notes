{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week2_ANN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMScPM9+1AqqmjUMygRDVtZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yitongknows/ML_Notes/blob/master/LectureNotes/Week2_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7inQd9jF_Upl"
      },
      "source": [
        "Date: Sept. 21, 2020 Monday \n",
        "## Artificial Neural Networks Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkGKj1wkEZM0"
      },
      "source": [
        "### Example 1 - Build a simple ANN algo\n",
        "Can think of a bias as a offset to the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVaOQ87OFIs5"
      },
      "source": [
        "import math\n",
        "\n",
        "# data (first column is the bias term)\n",
        "x = [[1, 0.1, -0.2],\n",
        "     [1, -0.1, 0.9],\n",
        "     [1, 1.2, 0.1],\n",
        "     [1, 1.1, 1.5]]\n",
        "\n",
        "# labels (desired output)\n",
        "t = [0, 0, 0, 1]\n",
        "\n",
        "# initial weights\n",
        "w = [1, -1, 1]\n",
        "# w = [-6, 2.6, 4]\n",
        "\n",
        "iteration = 50\n",
        "learning = 10\n",
        "\n",
        "def simple_ann(x, w, t, iteration, learning):\n",
        "  E = []\n",
        "\n",
        "  for ii in range(iteration):\n",
        "    y = []\n",
        "    err = []\n",
        "\n",
        "    for n in range(len(x)):\n",
        "      v = 0\n",
        "      #going through the columns in each row\n",
        "      # can think of them as features\n",
        "      for p in range(len(x[0])):\n",
        "        v = v + x[n][p] * w[p]\n",
        "      y.append(1 / (1 + math.e**(-v))) #sigmoidal activation\n",
        "\n",
        "      #adding the MSE\n",
        "      err.append((y[n] - t[n])**2) # MSE error\n",
        "\n",
        "      # gradient descent to compute new weights\n",
        "      for p in range(len(w)):\n",
        "        d = x[n][p]*(y[n]-t[n])*(1-y[n])*(y[n])\n",
        "        w[p] = w[p] - learning * d\n",
        "\n",
        "    E.append(2*sum(err)/len(x))\n",
        "\n",
        "  return (y, w, E)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdLMEvaxIZ-F",
        "outputId": "220fa3f8-479c-4cf5-a463-b94b54e5496c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "(y, w, E) = simple_ann(x, w, t, iteration, learning)\n",
        "print(y)\n",
        "print(w)\n",
        "print(E)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0006555859880223744, 0.052128479309599574, 0.04486688739303947, 0.9484229735074757]\n",
            "[-6.714065405238172, 2.6821640865189815, 4.525289511723171]\n",
            "[0.9216700394753333, 0.47174135286641483, 0.4301829615348312, 0.4421539206947274, 0.5517151805512346, 0.24024312038969337, 0.04117720770325407, 0.016367680329803912, 0.014618707903067763, 0.013421342040779494, 0.01246489244609104, 0.01166518326866741, 0.010979611629285312, 0.010381661072788465, 0.009853313343121933, 0.009381662074157875, 0.008957103933209306, 0.008572278345583679, 0.008221407302216046, 0.007899865061899094, 0.007603887061594066, 0.007330366731150998, 0.0070767098102673745, 0.006840727443246292, 0.006620556129589401, 0.006414596714760084, 0.006221467162903266, 0.006039965491063455, 0.005869040319786843, 0.005707767217182989, 0.005555329508569304, 0.0054110025695456995, 0.005274140865918842, 0.005144167181055287, 0.005020563600874198, 0.004902863922786668, 0.004790647226983471, 0.004683532403159226, 0.004581173467666569, 0.004483255538509642, 0.00438949136087997, 0.004299618295835624, 0.004213395700495847, 0.004130602640711687, 0.004051035887288632, 0.003974508155014073, 0.0039008465503923653, 0.0038298911994266503, 0.003761494031254582, 0.0036955176971346384]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSq6rCdNIvin"
      },
      "source": [
        "Note: the accuracy is only 50% with weights [1 -1 1]\n",
        "we can improve our algorithm by changing\n",
        "the weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2cN-AvNNoT4"
      },
      "source": [
        "### Lost Function for ANN\n",
        "\n",
        "Mean squared error (MSE):\n",
        "\n",
        "$$ Error = \\frac{1}{N} \\sum_{i=1}^{N}\n",
        "(y_i-t_i)^2 $$\n",
        "\n",
        "**$N$** is the number of training samples  \n",
        "**$y_i$** is the prediction value  \n",
        "**$t_i$** is the true value  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQnjT7dEPnCN"
      },
      "source": [
        "Note that there is another type of error call \"Cross Entropy\" (CE)\n",
        "\n",
        "$$ CE = -\\frac{1}{N}\\sum_{n=1}^{N}\\sum_\n",
        "{k=1}^Kt_{n,k}log(y_{n,k}) $$\n",
        "\n",
        "**$N$** is the number of training samples  \n",
        "**$K$** is the number of classes  \n",
        "**$y$** is the model predicted output  \n",
        "**$t$** is the target label  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42cpDCfbQyYv"
      },
      "source": [
        "### Weight Tuning Options\n",
        "\n",
        "\n",
        "*   Randomly picking it until it works\n",
        "*   Change one weight at a time in the direction that reduces error\n",
        "* **Gradient descent**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQhmN0zBRNRA"
      },
      "source": [
        "### Sigmoid (Logistic) Activation\n",
        "\n",
        "$$ \\sigma = \\frac{1}{1+e^{-x}} = \\frac{e^x}{e^x + 1}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPwkhmejSIPC"
      },
      "source": [
        "### Backpropagation: MSE\n",
        "$$ Error = \\frac{1}{N} \\sum_{i=1}^{N}\n",
        "(y_i-t_i)^2 $$\n",
        "\n",
        "**$N$** is the number of training samples  \n",
        "**$y_i$** is the prediction value  \n",
        "**$t_i$** is the true value\n",
        "\n",
        "**Gradient Descent**:\n",
        "\n",
        "$$ \\frac{dE}{dw_p} = \\frac{2}{N}(x_p)((y_n-t_n)(1-y)(y))) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-1LmB2XhPhn"
      },
      "source": [
        "### Tanh Activation\n",
        "\n",
        "$$ tanh=\\frac{e^x-e^{-x}}{e^x+e^{-x}}\n",
        "=\\frac{e^{2x}-1}{e^{2x}+1}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xPZwe7Dh8ir"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_wCMbg1JMc_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}